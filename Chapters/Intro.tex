
\section{Regression Verification}
It is very common that changes in a program introduce new unwanted behavior (`bugs'). Informal methods for checking the new code include code review, testing the new program, and \emph{regression testing}, a method by which the output of the new and old versions of the program are compared on a given set of test cases. Formal methods include program formal verification, which attempts to formally verify the correctness of the new program against user-specified assertions in the code, and formal program equivalence checking. Both problems are undecidable in general. While the general problem of proving equivalence of programs was studied for decades, mostly in the theorem proving world, the specialization of this problem to closely-related programs, coined by  
Godlin and Strichman as \emph{regression verification} \cite{DBLP:conf/dac/GodlinS09}, stirred the development of various tools and approaches, such as \tool{RVT}~\cite{DBLP:conf/dac/GodlinS09}, \tool{REVE}~\cite{DBLP:conf/kbse/FelsingGKRU14}, \tool{SymDiff}~\cite{DBLP:conf/cav/LahiriHKR12},\tool{ARDiff}~\cite{10.1145/3368089.3409757}, \tool{CLEVER}~\cite{10.1145/3238147.3238178}, \tool{ModDiff}~\cite{10.1007/978-3-319-66706-5_20}, \tool{IMP-S}~\cite{inproceedings} and \tool{DSE}~\cite{10.1145/1453101.1453131}. Regression verification suggests verifying the program against an older version of itself rather than verifying it against a user-supplied specification or assertions. That is, generate an automated proof of equivalence between a program and its previous version. Although this problem is still undecidable, for closely related programs regression verification is believed to be easier in practice than program verification. It spares the need to manually specify a specification and it can use the similarity between the programs to enable the proof  and to reduce the computational complexity~\cite{DBLP:conf/dac/GodlinS09}. The notion of the verification is weakened that way, as the last version might suffer from the same problems as the new one. Nevertheless, regression verification is relevant in the same places where regression testing is acceptable to ensure the code quality. Specifically, regression verification is most effective when the code has changed but supposes to behave the same toward its interfaces. This is true for \emph{refactoring}, where the code was rewritten to match a certain convention, and it is also true for \emph{performance tuning}. There is more than one definition to what equivalence between program means. In this article we will use the \emph{partial equivalence} notion \cite{DBLP:conf/dac/GodlinS09}. $P_1$ and $P_2$ are partially equivalent if given the same input, terminated executions of both programs yield the same output. Hereon, any reference to equivalence will refer to partial equivalence.

\subsection{Various Approaches to Regression Verification}
We will survey here several techniques that try to cope with the regression verification challenge.

REVE (stands for REgression VErification) \cite{DBLP:conf/kbse/FelsingGKRU14} aims to infer a \emph{coupling predicate} for a pair of programs $P_1$ and $P_2$, which is an invariant that relates $P_1$ and $P_2$ throughout their execution. They steer its generation so it will imply equality of the results in the case where they both terminate. If such a predicate can be inferred, they have proven equivalence between $P_1$ and $P_2$. To do so they compose a predicate containing function summaries, which are expressed as Horn clauses, loop and recursion invariants, and expressions containing the \emph{weakest liberal precondition} ($wlp$). \emph{wp($P,\varphi$)} denotes the weakest condition that needs to hold before an execution of statement list $P$ such that the execution terminates and the postcondition $\varphi$ holds in the final state~\cite{DBLP:journals/cacm/Dijkstra75}. For \emph{wlp($P,\varphi$)}, it demands that $\varphi$ holds after the execution of the program $P$ only if $P$ terminates. The invariants are synthesized automatically. Empirically, when the programs are closely related it is frequently enough to take the equivalence of related variables as the invariant. Then, they exhaustively use \emph{wlp calculus} to reduce the formula to a pure Horn clause. In the end they use a Horn constraint solver, such as Z3 \cite{DBLP:conf/sat/HoderB12} or Eldarica \cite{DBLP:conf/cav/RummerHK13}, to try to prove the equivalence or generate a counterexample if the programs are not equivalent or the proposed invariant is insufficient.

\tool{SYMDIFF} by Microsoft \cite{DBLP:conf/cav/LahiriHKR12} confronts the regression verification challenge from a different angle. Like \tool{REVE}, it is most effective in cases of closely related programs. It concentrates on the \emph{conditional equivalence} notion, which means that two programs are equivalent under a certain input condition. To do so, for each pair of functions it composes a procedure which assumes the input condition, executes both functions and asserts that their results are equal. It then reduces this program to Boogie, an intermediate language developed at Microsoft, and that code is sent to a program verification engine powered by Z3. \tool{SymDiff} also has an operational mode that imitates the one introduced by RVT~\cite{DBLP:conf/dac/GodlinS09}.

\tool{ARDiff} by Badihi at al.~\cite{10.1145/3368089.3409757} is the latest implementation of a regression verification tool based on symbolic execution - Å› a static analysis approach that reasons about program behaviors in terms of symbolic input variables. Noteworthy previous tools are \tool{CLEVER}~\cite{10.1145/3238147.3238178}, \tool{ModDiff}~\cite{10.1007/978-3-319-66706-5_20}, \tool{IMP-S}~\cite{inproceedings} and \tool{DSE}~\cite{10.1145/1453101.1453131}. One of the prominent advantages of using symbolic execution is that it is a top-down approach and therefore it is agnostic to the structure of each program and compare only their symbolic summaries. On the other hand, for programs containing loops and recursions, the number of paths is unlimited (unless given a loop invariant). Because of this, tools such as mentioned above can only prove equivalence up to a certain bound of iterations, and even so the number of paths may be exponential. To ease the verification burden, \tool{ARDiff} uses an abstraction/refinement loop based on CEGAR~\cite{10.1007/10722167_15}. It starts by replacing all syntactically-equal statements on both sides with uninterpreted functions, generates a symbolic summary to each program and uses as SMT-Solver to prove or refute equivalence. If the SMT SOLVER deems the program as non-equivalent, \tool{ARDiff} uses several heuristics to refine the program by restoring some of the abstracted statements and starts the described process again.


Every verification tool such as SeaHorn \cite{DBLP:conf/cav/GurfinkelKKN15}, HSF(C) \cite{DBLP:conf/tacas/GrebenshchikovGLPR12} and Automizer \cite{DBLP:conf/cav/HeizmannHP13}, as well as interactive theorem provers, can be used to prove equivalence between programs. It can be done by generating a new program that executes the given programs with the same non-deterministic input and asserts that their output is equal. However, this route does not take any advantage of the assumed similarity between the compared programs.

\section{Symbolic and Concolic Execution}
\label{sec:symbex}
\emph{Symbolic Execution} was first introduced by Boyer et al. in their tool \tool{SELECT}~\cite{10.1145/390016.808445} and its main incarnation nowadays is implemented \tool{KLEE}~\cite{10.5555/1855741.1855756}. Symbolic execution is an analysis technique where inputs are represented with symbolic values, and an interpreter is going through all the paths of the program and manage their symbolic states. Those states are used to assert and validate explicit (e.g., assert statements) and implicit (e.g., integer overflows) properties of the given program. Different approaches try to cope with the problem of recursions and loops which create infinite paths (unless the program explicitly block the number of iterations), but eventually every implementation has a predefined timeout. As such, it is wildly used to find bugs and generate real-life test cases. A known problem associated with symbolic execution is the ``path explosion" problem~\cite{10.1007/978-3-540-78800-3_28}, which state that even in the cases of bounded number of paths, they are exponential in the branching statements of the program. To improve the code coverage, \emph{Concolic Execution} (a portmanteau of \emph{concrete} and \emph{symbolic}) - introduced in \tool{DART}~\cite{10.1145/1064978.1065036} -  uses concrete values to refine path conditions (the conjunction of all the visited branchings' predicates in the symbolic execution) and to avoid reaching unreachable states. Later on in this thesis we will show a tool that implement this technique to generate whitebox test cases.

\section{Thesis outline}
\label{sec:outline}
In this thesis, I discuss methods to improve the incompleteness of the proof rules used by \tool{RVT}. 

Chapter \ref{chptr:rec} presents the first contribution of my thesis. This chapter examines the problem of proving equivalence of functions that has more than one recursive call. We start by addressing recursions that have only a single path with a recursive calls, and move on to multiple paths with recursive calls. Then, we suggest a novel method for proving the equivalence of such functions by partitioning the input domain and prove each partition with fitting methods. Another contribution of this chapter is a technique to automatically find an unrolling that synchronize the given functions. How we take advantage of this unrolling to validate the equivalence of the pair is explained thoroughly. We continue to show that our method is capable of proving pairs that can't be proved by any other tool and therefore is state-of-the-art, and finish by demonstrating some of its limitations.

Chapter \ref{chptr:integ} discusses the second contribution of my thesis - an algorithm to integrate \tool{RVT} with other tools. Due to the undecidable nature of regression verification, each solution tackles it from another direction and succeed in other areas of the problem. One can simply run several tools consecutively on a given pair of programs, but this all-or-nothing approach does not leverage the fact that different functions in the programs can be proved equivalent with different proof rules (that are implemented in different tools). We present here a method which preserves the decomposition algorithm of \tool{RVT} but also uses other tools when \tool{RVT} fails and show examples that is proved with our new algorithm.

